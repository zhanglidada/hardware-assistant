#!/usr/bin/env python3
"""
æ•°æ®ç®¡é“å·¥å…·å‡½æ•°
æä¾›æ—¥å¿—ã€éªŒè¯ã€å¤‡ä»½ç­‰é€šç”¨åŠŸèƒ½
"""

import os
import json
import shutil
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional

from config import LOG_CONFIG, BACKUP_CONFIG, VALIDATION_CONFIG


class Logger:
    """ç»Ÿä¸€çš„æ—¥å¿—ç®¡ç†å™¨"""
    
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialize()
        return cls._instance
    
    def _initialize(self):
        """åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ"""
        self.logger = logging.getLogger('DataPipeline')
        self.logger.setLevel(getattr(logging, LOG_CONFIG["level"]))
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_formatter = logging.Formatter('%(message)s')
        console_handler.setFormatter(console_formatter)
        self.logger.addHandler(console_handler)
        
        # æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if LOG_CONFIG["enabled"]:
            # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
            log_file = Path(LOG_CONFIG["file"])
            log_file.parent.mkdir(parents=True, exist_ok=True)
            
            file_handler = logging.FileHandler(
                LOG_CONFIG["file"], 
                encoding='utf-8'
            )
            file_handler.setLevel(logging.DEBUG)
            file_formatter = logging.Formatter(
                '%(asctime)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            file_handler.setFormatter(file_formatter)
            self.logger.addHandler(file_handler)
    
    def info(self, message: str):
        """è®°å½•ä¿¡æ¯æ—¥å¿—"""
        self.logger.info(message)
    
    def debug(self, message: str):
        """è®°å½•è°ƒè¯•æ—¥å¿—"""
        self.logger.debug(message)
    
    def warning(self, message: str):
        """è®°å½•è­¦å‘Šæ—¥å¿—"""
        self.logger.warning(message)
    
    def error(self, message: str):
        """è®°å½•é”™è¯¯æ—¥å¿—"""
        self.logger.error(message)


# å…¨å±€æ—¥å¿—å®ä¾‹
logger = Logger()


class DataValidator:
    """æ•°æ®éªŒè¯å™¨"""
    
    @staticmethod
    def validate_item(item: Dict[str, Any], data_type: str) -> tuple[bool, Optional[str]]:
        """
        éªŒè¯å•ä¸ªæ•°æ®é¡¹
        
        Args:
            item: æ•°æ®é¡¹
            data_type: æ•°æ®ç±»å‹ï¼ˆcpu/gpu/phoneï¼‰
            
        Returns:
            (æ˜¯å¦é€šè¿‡, é”™è¯¯ä¿¡æ¯)
        """
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field in VALIDATION_CONFIG["required_fields"]:
            if field not in item:
                return False, f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"
        
        # æ£€æŸ¥IDæ ¼å¼
        if not isinstance(item.get("id"), str) or not item["id"]:
            return False, "IDå¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²"
        
        # æ£€æŸ¥ä»·æ ¼èŒƒå›´
        if VALIDATION_CONFIG["check_price_range"]:
            price = item.get("price", 0)
            if not isinstance(price, (int, float)):
                return False, "ä»·æ ¼å¿…é¡»æ˜¯æ•°å­—"
            if price < VALIDATION_CONFIG["min_price"] or price > VALIDATION_CONFIG["max_price"]:
                return False, f"ä»·æ ¼è¶…å‡ºåˆç†èŒƒå›´: {price}"
        
        return True, None
    
    @staticmethod
    def validate_data_list(data: List[Dict[str, Any]], data_type: str) -> tuple[bool, List[str]]:
        """
        éªŒè¯æ•°æ®åˆ—è¡¨
        
        Args:
            data: æ•°æ®åˆ—è¡¨
            data_type: æ•°æ®ç±»å‹
            
        Returns:
            (æ˜¯å¦å…¨éƒ¨é€šè¿‡, é”™è¯¯åˆ—è¡¨)
        """
        if not data:
            return False, ["æ•°æ®åˆ—è¡¨ä¸ºç©º"]
        
        errors = []
        ids_seen = set()
        
        for i, item in enumerate(data):
            # éªŒè¯å•ä¸ªé¡¹ç›®
            is_valid, error = DataValidator.validate_item(item, data_type)
            if not is_valid:
                errors.append(f"ç¬¬{i+1}é¡¹: {error}")
                continue
            
            # æ£€æŸ¥IDå”¯ä¸€æ€§
            if VALIDATION_CONFIG["check_duplicates"]:
                item_id = item["id"]
                if item_id in ids_seen:
                    errors.append(f"ç¬¬{i+1}é¡¹: é‡å¤çš„ID: {item_id}")
                ids_seen.add(item_id)
        
        return len(errors) == 0, errors


class BackupManager:
    """å¤‡ä»½ç®¡ç†å™¨"""
    
    @staticmethod
    def create_backup(file_path: Path, backup_dir: Path) -> Optional[Path]:
        """
        åˆ›å»ºæ–‡ä»¶å¤‡ä»½
        
        Args:
            file_path: æºæ–‡ä»¶è·¯å¾„
            backup_dir: å¤‡ä»½ç›®å½•
            
        Returns:
            å¤‡ä»½æ–‡ä»¶è·¯å¾„
        """
        if not BACKUP_CONFIG["enabled"]:
            return None
        
        if not file_path.exists():
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å¤‡ä»½: {file_path}")
            return None
        
        # åˆ›å»ºæ—¥æœŸç›®å½•
        today = datetime.now().strftime("%Y%m%d")
        backup_date_dir = backup_dir / today
        backup_date_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{file_path.stem}_{timestamp}.json"
        backup_path = backup_date_dir / backup_name
        
        try:
            shutil.copy2(file_path, backup_path)
            logger.info(f"âœ… å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_path}")
            return backup_path
        except Exception as e:
            logger.error(f"âŒ å¤‡ä»½åˆ›å»ºå¤±è´¥: {e}")
            return None
    
    @staticmethod
    def cleanup_old_backups(backup_dir: Path):
        """
        æ¸…ç†è¿‡æœŸå¤‡ä»½
        
        Args:
            backup_dir: å¤‡ä»½ç›®å½•
        """
        if not BACKUP_CONFIG["auto_cleanup"]:
            return
        
        cutoff_date = datetime.now() - timedelta(days=BACKUP_CONFIG["keep_days"])
        cutoff_str = cutoff_date.strftime("%Y%m%d")
        
        try:
            for date_dir in backup_dir.iterdir():
                if date_dir.is_dir() and date_dir.name.isdigit():
                    if date_dir.name < cutoff_str:
                        shutil.rmtree(date_dir)
                        logger.info(f"ğŸ—‘ï¸  æ¸…ç†è¿‡æœŸå¤‡ä»½: {date_dir}")
        except Exception as e:
            logger.error(f"âš ï¸  æ¸…ç†å¤‡ä»½å¤±è´¥: {e}")


class DataComparator:
    """æ•°æ®å¯¹æ¯”å™¨"""
    
    @staticmethod
    def compare_data(old_data: List[Dict], new_data: List[Dict]) -> Dict[str, Any]:
        """
        å¯¹æ¯”æ–°æ—§æ•°æ®ï¼Œç”Ÿæˆå˜æ›´ç»Ÿè®¡
        
        Args:
            old_data: æ—§æ•°æ®åˆ—è¡¨
            new_data: æ–°æ•°æ®åˆ—è¡¨
            
        Returns:
            å˜æ›´ç»Ÿè®¡å­—å…¸
        """
        old_ids = {item["id"]: item for item in old_data}
        new_ids = {item["id"]: item for item in new_data}
        
        old_id_set = set(old_ids.keys())
        new_id_set = set(new_ids.keys())
        
        # è®¡ç®—å˜æ›´
        added = new_id_set - old_id_set
        removed = old_id_set - new_id_set
        common = old_id_set & new_id_set
        
        # æ£€æŸ¥æ›´æ–°çš„é¡¹ç›®
        updated = set()
        for item_id in common:
            if old_ids[item_id] != new_ids[item_id]:
                updated.add(item_id)
        
        unchanged = common - updated
        
        return {
            "total_new": len(new_data),
            "total_old": len(old_data),
            "added": len(added),
            "removed": len(removed),
            "updated": len(updated),
            "unchanged": len(unchanged),
            "added_ids": list(added),
            "removed_ids": list(removed),
            "updated_ids": list(updated)
        }
    
    @staticmethod
    def print_comparison(data_type: str, stats: Dict[str, Any]):
        """
        æ‰“å°å¯¹æ¯”ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            data_type: æ•°æ®ç±»å‹
            stats: ç»Ÿè®¡æ•°æ®
        """
        logger.info(f"\nğŸ“Š  {data_type.upper()}æ•°æ®æ›´æ–°ç»Ÿè®¡:")
        logger.info(f"   æ€»è®¡é¡¹ç›®: {stats['total_new']} (ä¹‹å‰: {stats['total_old']})")
        logger.info(f"   æ–°å¢é¡¹ç›®: {stats['added']}")
        logger.info(f"   åˆ é™¤é¡¹ç›®: {stats['removed']}")
        logger.info(f"   æ›´æ–°é¡¹ç›®: {stats['updated']}")
        logger.info(f"   æœªå˜é¡¹ç›®: {stats['unchanged']}")
        
        if stats['added'] > 0 and stats['added_ids']:
            logger.debug(f"   æ–°å¢ID: {', '.join(stats['added_ids'][:5])}...")
        if stats['removed'] > 0 and stats['removed_ids']:
            logger.debug(f"   åˆ é™¤ID: {', '.join(stats['removed_ids'][:5])}...")


def save_json(data: List[Dict], file_path: Path) -> bool:
    """
    ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶
    
    Args:
        data: æ•°æ®åˆ—è¡¨
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        file_path.parent.mkdir(parents=True, exist_ok=True)
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ… æ•°æ®ä¿å­˜æˆåŠŸ: {file_path}")
        return True
    except Exception as e:
        logger.error(f"âŒ æ•°æ®ä¿å­˜å¤±è´¥: {e}")
        return False


def load_json(file_path: Path) -> Optional[List[Dict]]:
    """
    ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ•°æ®åˆ—è¡¨æˆ–None
    """
    if not file_path.exists():
        return None
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None
